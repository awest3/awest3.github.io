[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Inference Tutorials @ ANU",
    "section": "",
    "text": "Tutorial Questions\nThese are the tutorial questions for the course. Most have come or are based on Statistical Inference by Garthwaite, Jolliffe, and Jones.",
    "crumbs": [
      "Tutorial Questions"
    ]
  },
  {
    "objectID": "Tut1.html",
    "href": "Tut1.html",
    "title": "1  Tutorial 1 (Week 2)",
    "section": "",
    "text": "Let \\(X\\) and \\(Y\\) be discrete random variables each taking values in the sample space \\(S = \\{0, 1, 2\\}\\) and having a joint probability mass function (pmf) given by the following table:\n\n\nFind the pmf and cumulative distribution function (CDF) of \\(U = X + Y\\).\nFind the marginal pmf’s of both \\(X\\) and \\(Y\\). Are \\(X\\) and \\(Y\\) independent?\nLet \\(X_1\\) be a discrete random variable having a pmf equal to the marginal pmf of \\(X\\) calculated in part (b). Similarly, let \\(Y_1\\) be a discrete random variable having a pmf equal to the marginal pmf of Y calculated in part (b). Also, let \\(X_1\\) and \\(Y_1\\) be independent. Construct a table similar to the one above giving the joint pmf of \\(X_1\\) and \\(Y_1\\).\nUsing your result from part (c), calculate the pmf of the random variable \\(U_1 = X_1 + Y_1\\). Compare this pmf with the one you calculated in part (a).\nCompute \\(E(X)\\) and \\(Var(Y)\\).\nCalculate the pmf of the random variable \\(E(X|Y)\\). In other words, find \\(E(X|Y = y)\\) and calculate \\(E(X|Y = y)\\) for each of \\(y = 0, 1, 2\\). Verify the identity \\(E(X) = E\\{E(X|Y)\\}\\) for these two random variables.\n\n\nLet \\(X\\) be normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Find the moment generating function (mgf) of \\(X\\), \\(M_{X(t)} = E(e^{tX})\\). [HINT: Write the integral definition of the required expectation and then “complete-the-square” in the exponent. Recall that:\n\n\\[\\int_{-\\infty}^{\\infty} \\frac{1}{b \\sqrt{2 \\pi}} exp\\left\\{ - \\frac{1}{2 b^2} (x - a)^2 \\right\\} dx = 1,\\]\nfor any values \\(a\\) and \\(b &gt; 0\\), since this is just the integral of the normal density with mean \\(a\\) and variance \\(b^2\\) over its entire range.]\n\nLet \\(X\\) be a continuous random variable having a pdf of the form:\n\n\\[f(x) = \\sqrt{\\frac{2}{\\pi}} \\exp \\left\\{ - \\frac{1}{2} x^2 \\right\\}, \\ \\  \\textrm{x &gt; 0}.\\]\nThe distribution having this density is often referred to as the “folded normal”.\n\nLet \\(Y = X^2\\). Find the pdf of \\(Y\\).\nDo you recognise the density you found in part (a)?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Tutorial 1 (Week 2)</span>"
    ]
  },
  {
    "objectID": "Tut2.html",
    "href": "Tut2.html",
    "title": "2  Tutorial 2 (Week 3)",
    "section": "",
    "text": "Consider the following questions from Statistical Inference by Garthwaite, Joliffe, and Jones (for the course the questions may be edited and/or sightly modified):\n\n\nQ 2.1: Suppose \\(X_1, X_2, \\ldots, X_n \\sim \\textrm{Uniform} [0, \\theta]\\).\n\nFind the pdf of \\(X_{(n)}\\), the largest of the \\(X_{i}\\).\nShow that \\(2\\bar{X}\\) and \\(\\frac{n+1}{n}X_{(n)}\\) are both consistent estimators of \\(\\theta\\) and compare their variances.\n\nQ 2.2: Suppose that \\(\\hat{\\theta}_1\\) and \\(\\hat{\\theta}_2\\) are independent unbiased estimators of \\(\\theta\\), with variances of \\(\\sigma^2_1\\), \\(\\sigma^2_2\\), respectively. Also consider \\(\\tilde{\\theta} = k_1 \\hat{\\theta}_1 +  k_2 \\hat{\\theta}_2\\), where \\(k_1\\) and \\(k_2\\) are constants. Find the values \\(k_1\\) and \\(k_2\\) so that \\(\\tilde{\\theta}\\) is unbiased and has the smallest possible variance.\n\n\nOn Wattle there is the data for 2020 Gross Domestic Product (GDP) https://data.worldbank.org. The World Bank states:\n\n\nGDP at purchaser’s prices is the sum of gross value added by all resident producers in the economy plus any product taxes and minus any subsidies not included in the value of the products. It is calculated without making deductions for depreciation of fabricated assets or for depletion and degradation of natural resources. Data are in current U.S. dollars. Dollar figures for GDP are converted from domestic currencies using single year official exchange rates. For a few countries where the official exchange rate does not reflect the rate effectively applied to actual foreign exchange transactions, an alternative conversion factor is used.\n\n\nConsider the following: a. Visually display the data and discuss. Try taking the natural log of the data (when statisticians say “log” they mean natural log).\n\n\nCompute a six number summary of the data.\n\nBased on the ``box plot rule’’, determine if there are any outliers. Which countries are outliers? To use the rule examine the following: Are any values in the data below the \\(1^{\\textrm{st}}\\) Quartile - 1.5 IQR? Are any values in the data above the \\(3^{\\textrm{rd}}\\) Quartile + 1.5 IQR? IQR is the inter-quartile range.\n\nLet \\(Y_{i}\\) = log(GDP_i). Suppose \\(Y_1, \\ldots, Y_n  \\sim \\textrm{ i.i.d } \\textrm{normal}(\\mu, \\sigma^2)\\). What is your best guess for \\(\\mu\\) and \\(\\sigma^2\\) as functions of \\(Y\\) (call these \\(T_1\\) and \\(T_2\\))? What are the means (expected values) of \\(T_1\\) and \\(T_2\\)?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tutorial 2 (Week 3)</span>"
    ]
  },
  {
    "objectID": "Tut3.html",
    "href": "Tut3.html",
    "title": "3  Tutorial 3 (Week 4)",
    "section": "",
    "text": "Answer from Statistical Inference by Garthwaite, Joliffe, and Jones (for the course the questions may be edited and/or sightly modified):\n\n\nQ 2.6: Consider \\(X_1, X_2, \\ldots, X_n \\sim iid \\textrm{ Bernouilli}(\\theta)\\).\n\nDetermine the Cramer-Rao lower bound for an unbiased estimator of \\(\\theta\\).\nDetermine the Cramer-Rao lower bound for an unbiased estimator of \\(\\theta^2\\).\n\nQ 2.8: Consider \\(X_1, X_2, \\ldots, X_n\\) being a random sample from a normal distribution with known mean (\\(\\mu\\)) and unknown variance (\\(\\sigma^2\\)).\n\nShow that the sample variance:\n\n\\[S^2 = \\frac{\\sum_{i=1}^n \\left(X_i - \\bar{X} \\right)^2}{n-1}\\] does not attain the Cramer-Rao lower bound for finite \\(n\\), but does so as \\(n\\) tends to infinity.\n\nFor what value of \\(c\\) does the estimator of \\(\\sigma^2\\) have the smallest mean squared error?\n\n\\[c\\sum_{i=1}^n \\left(X_i - \\bar{X} \\right)^2\\]\nQ 2.10: Consider \\(X_1, X_2, \\ldots, X_n\\) being a random sample from a normal distribution with unknown mean (\\(\\theta\\)) and known variance (\\(\\sigma^2\\)). Determine the Cramer-Rao lower bound for an unbiased estimator of \\(\\theta\\).\n\n\n(from John Rice): Use the Monte Carlo method with \\(n=100\\) and \\(n=1000\\) to estimate:\n\n\\[\\int_0^1 cos(2 \\pi x) dx\\]\nCompare it with the exact answer.\n\n(from John Rice): Use the Monte Carlo method with \\(n=100\\) and \\(n=1000\\) to estimate:\n\n\\[\\int_0^1 cos(2 \\pi x^2) dx\\]\nNo exact answer (i.e. closed form analytical solution) exists.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tutorial 3 (Week 4)</span>"
    ]
  },
  {
    "objectID": "Tut4.html",
    "href": "Tut4.html",
    "title": "4  Tutorial 4 (Week 5)",
    "section": "",
    "text": "Answer the following from CASI (for the course the questions may be edited and/or sightly modified):\n\n\n2.1: A coin with probability of heads \\(\\theta\\) is independently flipped \\(n\\) times, after which \\(\\theta\\) is estimated by\n\\[\\hat{\\theta} = \\frac{s+1}{n+2},\\] with \\(s\\) equal the number of heads observed.\n\nWhat are the bias and variance of \\(\\hat{\\theta}\\)?\nHow would you apply the plug-in principle to get a practical estimate of se(\\(\\hat{\\theta}\\))?\n\n\n\nAnswer the following from Statistical Inference by Garthwaite, Joliffe, and Jones (for the course the questions may be edited and/or sightly modified):\n\n\nQ 2.14: Suppose \\(X_1, \\ldots. X_n \\iid \\textrm{beta}(a, b)\\). Where:\n\\[f(x; a, b) = \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)} x^{a-1} (1-x)^{b-1}; \\ \\ 0 &lt; x &lt; 1, \\ \\ a,b &gt;0.\\]\n\nUsing the factorization method, determine the sufficient statistics for \\(a\\) and \\(b\\).\n\nUsing the ratio method, determine the minimal sufficient statistics for \\(a\\) and \\(b\\).\n\nQ 2.13: Find minimal sufficient statistics for a independent samples of size \\(n\\) from the following distributions:\n\nthe uniform distribution on \\(\\left[\\theta - \\frac{1}{2}, \\theta + \\frac{1}{2}\\right]\\).\nthe uniform distribution on \\(\\left[-\\theta, \\theta\\right]\\).\n\n\n\nLet \\(U \\sim \\textrm{uniform}(0,1)\\).\n\nShow that both \\(-\\log(U)\\) and \\(-\\log(1-U)\\) are exponential random variables.\nShow that \\(\\log\\left( \\frac{u}{1-u} \\right)\\) is a logistic(0,1) random variable.\nShow how to generate samples from a logistic(\\(\\mu\\), \\(\\beta\\)).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tutorial 4 (Week 5)</span>"
    ]
  },
  {
    "objectID": "Tut5.html",
    "href": "Tut5.html",
    "title": "5  Tutorial 5 (Week 6)",
    "section": "",
    "text": "Answer the following from Statistical Inference by Garthwaite, Joliffe, and Jones (for the course the questions may be edited and/or sightly modified):\n\n\nQ 2.21: Suppose \\(X_1, \\ldots. X_n \\iid \\textrm{exponential}(\\theta)\\), where\n\\[f(x; \\theta) = \\frac{1}{\\theta} \\exp\\{-x/\\theta\\}, \\ \\ x \\geq, \\ \\ \\theta &gt; 0.\\] Consider the following estimators of \\(\\theta\\); \\(T_1 =\\sum X_i/n\\), \\(T_2 =\\sum X_i/(n+1)\\), and \\(t_3 = nY\\), where \\(Y = \\textrm{minimum}(X_1, X_2, \\ldots, X_3)\\).\n\nWhich of these estimators are unbiased, which are functions of sufficient statistics, and which are consistent?\n\nDiscuss the relative merits of \\(T_1, T_2\\), and \\(T_3\\) with regard to the above and any other criteria.\n\nQ 3.1: Find the MLE for \\(\\theta\\) in the following cases, where each case we have a random sample \\(X_1, X_2, \\ldots, X_n\\) from the relevant distributions.\n\nThe geometric distribution with probability function \\(\\theta (1-\\theta)^{x-1}\\), \\(x=1,2,\\ldots\\).\nThe uniform distribution on the interval \\((0, \\theta)\\).\nThe gamma distribution with parameters \\(2\\) and \\(\\theta\\) with p.d.f:\n\n\\[f(x;\\theta) = \\frac{1}{\\theta^2} x \\exp\\{-x/\\theta\\}, \\ \\ x&gt;0.\\]\n\nThe Poisson distribution with mean \\(\\theta\\).\n\nQ 3.11: Let \\(X_1, \\ldots, X_n \\iid \\textrm{Poisson}(\\theta)\\). Suppose \\(\\phi=\\theta^2\\). Find the MLE for \\(\\phi\\), and show that the estimator \\(\\hat{\\phi}\\) is a biased but consistent estimator. Note: \\(E[X^4]= \\theta^4 + 6\\theta^3 + 7 \\theta^2 + \\theta\\).\n\n\nBased on Question 3.1 (c), using the following data:\n\n\nset.seed(1001)\nn &lt;- 100\ny &lt;- rgamma(n, 2, scale=5)\n\n\nCalculate the estimate \\(\\hat{\\theta}\\) analytically.\nCode up a Newton-Raphson routine to compute \\(\\hat{\\theta}\\).\nUse in R to compute \\(\\hat{\\theta}\\).\n\n\nLet \\(X \\sim \\textrm{uniform}(0, \\theta); \\ \\theta &gt; 0\\) . Suppose:\n\n\\[0 = E[g(X)] = \\int_0^{\\theta} g(x) \\frac{1}{\\theta} dx, \\ \\ \\forall \\ \\theta.\\] Then \\(0 = \\int_0^{\\theta} g(x) dx\\) for all \\(\\theta\\). Then \\(g(x) = 0\\) for all \\(x\\). Now consider \\(X_1, \\ldots, X_n \\iid \\textrm{uniform}(0, \\theta)\\), and the statistic \\(T(\\bs{X}) = \\textrm{max}(X_1, \\ldots, X_n)\\). Show that \\(T(X)\\) is complete.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tutorial 5 (Week 6)</span>"
    ]
  },
  {
    "objectID": "Tut6.html",
    "href": "Tut6.html",
    "title": "6  Tutorial 6 (Week 7)",
    "section": "",
    "text": "Answer the following from Statistical Inference by Garthwaite, Joliffe, and Jones (for the course the questions may be edited and/or sightly modified):\n\n\nQ 3.17:\n\nThe gamma distribution \\(f(x; \\alpha, \\beta) = \\frac{\\beta^{\\alpha} x^{\\alpha-1} \\exp\\{-\\beta x\\}}{\\Gamma(\\alpha)}, x &gt; 0,\\) has mean \\(\\alpha/\\beta\\) and variance \\(\\alpha/\\beta^2\\). If \\(\\alpha\\) is known, find the MLE of \\(\\beta\\) and its asymptotic variance. Additionally, determine an unbiased estimator of \\(\\beta\\) and determine whether it obtains the CRLB. Also find the MLE for \\(1/\\beta\\). What is its mean and variance? Does this estimator attain the CRLB?\nVerify that the MLE of \\(\\beta\\) is sufficient. Is the MLE of \\(1/\\beta\\) also sufficient?\n\nQ 3.23: A cosmetic company is considering the marketing of a new product for men and wishes to estimate the proportion, \\(\\theta\\), of males in a certain age group that would buy the product. As a direct question may cause embarrassment a so-called randomized response procedure is used to disguise the interviewee’s actual willingness to buy the product.\n\nEach person interviewed throws a fair die, and instead of giving the interviewer his true response Yes (will buy the product)' ofNo (will not buy it)’, he gives a coded response \\(A, B,\\) or \\(C\\) as indicated by the table. The interviewer does not see the score on the die.\nIn a random sample of 1,000 men, the numbers of A, B, C responses were 440, 310, and 250, respectively. If each man in the sample has the same probability, \\(\\theta\\) of having the response `Yes’, show that the log-likelihood fro \\(\\theta\\) is:\n\\[440 \\log(3-\\theta) + 310 \\log(2-\\theta) + 250 \\log(1+2 \\theta) + \\textrm{constant},\\] and obtain the maximum likelihood estimate of \\(\\theta\\). Additionally provide the asymptotic distribution of the MLE.\n\nQ 3.26. Find the method-of-moments estimators for \\(\\theta\\) from Q 3.1 in Tutorial 5, except for (b) let \\(X_1, \\ldots, X_n \\iid  \\textrm{Uniform}(-\\theta/2, \\theta/2)\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tutorial 6 (Week 7)</span>"
    ]
  },
  {
    "objectID": "Tut7.html",
    "href": "Tut7.html",
    "title": "7  Tutorial 7 (Week 8)",
    "section": "",
    "text": "[based on GJJ Q 3.17]. Suppose \\(X_1, \\ldots, X_n \\iid \\textrm{gamma}(a, b)\\) where \\(E[X]= a b\\). Suppose that the data on on gamma-rays (measuring the inter-arrival times of 3,935 photons (units in seconds) can be modeled by a gamma distribution.\n\nMake a histogram of the data. The data are on Wattle.\nDetermine the method of moments estimators for \\(a\\) and \\(b\\).\nDetermine the MLEs for \\(a\\) and \\(b\\). You will need to use the and functions in .\nPlace the fitted densities (based on both estimators) on the histogram.\n\nConsider a random sample of twins pairs. Twin pairs may be identical or fraternal. Let \\(u\\) of these pairs consist of male pairs, \\(v\\) consist of female pairs, and \\(w\\) consist of opposite sex pairs. A simple model for these data is based on a Bernoulli distribution for each pair dictating whether it consists of identical or fraternal twins. Suppose that identical twins occur with probability \\(p\\) and fraternal twins with probability \\(1-p\\). Once the decision is made as to whether the twins are identical or not, then sexes are are assigned to the twins. If the twins are identical, then one assignment of sex is made. If the twins are non-identical, then two independent assignments of sex are made. Suppose boys are chosen with probability \\(q\\) and girls with probability \\(1-q\\).\n\nWrite out the likelihood for these data.\nFormulate a “missing data” model and specify both steps of the EM algorithm to obtain the MLEs for \\(p\\) and \\(q\\).\nCarry out the algorithm for \\(u=22\\), \\(v=21\\), and \\(w=25\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tutorial 7 (Week 8)</span>"
    ]
  },
  {
    "objectID": "Tut8.html",
    "href": "Tut8.html",
    "title": "8  Tutorial 8 (Week 9)",
    "section": "",
    "text": "Answer the following from Statistical Inference by Garthwaite, Joliffe, and Jones (for the course the questions may be edited and/or sightly modified):\n\n\nQ 4.1: Use the Neyman-Pearson lemma to outline the following tests for a best test of \\(H_0\\) against \\(H_1\\):\n\n\\(X_1, \\ldots X_n \\iid \\textrm{Poisson}(\\theta)\\). Test \\(H_0: \\theta=\\theta_0\\) against \\(H_1: \\theta=\\theta_1, \\ \\theta_1 &gt;\\theta_0\\).\n\\(X_1, \\ldots X_n \\iid \\textrm{exponential}(\\theta)\\), where \\(E[X] = \\theta\\). Test \\(H_0: \\theta=\\theta_0\\) against \\(H_1: \\theta=\\theta_1, \\ \\theta_1 &gt;\\theta_0\\).\n\\(X_1, \\ldots X_n \\iid \\textrm{exponential}(\\theta)\\), where \\(E[X] = 1/\\theta\\). Test \\(H_0: \\theta=\\theta_0\\) against \\(H_1: \\theta=\\theta_1, \\ \\theta_1 &gt;\\theta_0\\).\n\\(X_{11}, \\ldots X_{1n_1} \\iid \\textrm{normal}(\\mu_1, \\sigma^2_1)\\), and \\(X_{21}, \\ldots X_{2n_2} \\iid \\textrm{normal}(\\mu_2, \\sigma^2_2)\\). All \\(X_{ij}\\) are independent of each other and \\(\\sigma^2_1, \\sigma^2_2\\) are known. Test \\(H_0: \\mu_2=\\mu_1\\) against \\(H_1: \\mu_2 = \\mu_1 + \\delta, \\ \\delta&gt;0\\) and \\(\\delta\\) is a known constant.\n\nQ 4.2: In the previous question part (d), suppose that \\(\\sigma^2_1 = \\sigma^2_2 = \\delta=1, n_1=n_2=n\\) and that we wish to perform a best test with \\(\\alpha=0.01\\). Find:\n\nthe power of the test when \\(n=10\\);\nthe smallest value for \\(n\\), for which we can achieve a power \\(geq 0.95\\).\n\nQ 4.3: \\(X_1, \\ldots X_n \\iid \\textrm{gamma}(\\lambda, \\theta)\\) with p.d.f:\n\\[f(x; \\theta) = \\frac{\\theta^{\\lambda}}{\\Gamma(\\lambda)} x^{\\lambda-1} \\exp\\{ - \\theta x \\},\\ x&gt;0,\\] where \\(\\lambda&gt;0\\) is known, but \\(\\theta&gt;0\\) is unknown. Show that the critical region for the most powerful \\(\\alpha\\)-level test of \\(H_0: \\theta=\\theta_0\\) against \\(H_1: \\theta=\\theta_1, \\ \\theta_1 &gt;\\theta_0\\) is of the form \\(\\sum_{i=1}^n X_i \\leq C\\), for some constant \\(C\\). Also show that when \\(n=1\\) and \\(\\lambda=1\\), the power of this test is \\(1- (1-\\alpha)^{\\theta_1/\\theta_0}\\).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tutorial 8 (Week 9)</span>"
    ]
  },
  {
    "objectID": "Tut9.html",
    "href": "Tut9.html",
    "title": "9  Tutorial 9 (Week 10)",
    "section": "",
    "text": "From Statistical Inference by Garthwaite, Joliffe, and Jones see Example 4.8 (T-test pages 1 and 2). Simply write out the steps as outlined in the example. This is the classic t-test.\nConsider a Poisson regression model using the canonical link function (how do we determine the canonical link function?):\n\n\\[\\begin{eqnarray*}\nY_1, \\ldots, Y_n &\\ind& \\textrm{Poisson}(\\lambda_i) \\\\\n\\textrm{log}(\\lambda_i)  &=& \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 \\\\\n\\lefteqn{\\textrm{ for } i = 1, \\ldots, n.}\n\\end{eqnarray*}\\]\n\nUsing and the data on the website to find the MLEs: \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2\\), as well as their estimated asymptotic variances.\nAdditionally, in the solutions a bootstrap procedure for the coefficients will be outlined. You do not need to run this procedure before the tutorial. The bootstrap will be discussed later in the course.\nData: A sample from a population of 52 female song sparrows was studied over the course of a summer and their reproductive activities were recorded. In particular, the age and number of new offspring were recorded for each sparrow (Arcese et al, 1992). Let \\(Y = \\textrm{fledged}\\) (number of offspring), and \\(X =  \\textrm{age}\\) (age of mother).\n\n\nFrom Statistical Inference by Garthwaite, Joliffe, and Jones answer the following (for the course the questions may be edited and/or sightly modified):\n\n\nQ 4.7: A random sample of \\(X_1, X_2, \\ldots, X_n\\) is taken from the following density:\n\\[f(x; \\theta) = \\frac{1}{(2 \\pi)^{1/2} \\theta x} \\exp \\left\\{ -\\frac{1}{2} \\left(\\frac{\\ln(x)}{\\theta}\\right)\\right\\},\\  x&gt;0, \\ \\theta&gt;0.\\]\n\nShow that there is a UMP test of the null hypothesis \\(H_0: \\theta=\\theta_0\\) versus \\(H_1: \\theta&gt;\\theta_0\\), and find the form of the test.\n\nQ 4.19: Using a random sample of size \\(n\\) from a Poisson distribution with mean \\(\\theta\\), consider testing \\(H_0: \\theta=\\theta_0\\) versus \\(H_1:\\theta \\not= \\theta_0\\). Find the test statistic for (a) a score test, and (b) a Wald test.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Tutorial 9 (Week 10)</span>"
    ]
  },
  {
    "objectID": "Tut10.html",
    "href": "Tut10.html",
    "title": "10  Tutorial 10 (Week 11)",
    "section": "",
    "text": "Consider a Poisson regression model using the canonical link function (how do we determine the canonical link function?):\n\n\\[\\begin{eqnarray*}\nY_1, \\ldots, Y_n &\\ind& \\textrm{Poisson}(\\lambda_i) \\\\\n\\textrm{log}(\\lambda_i)  &=& \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 \\\\\n\\lefteqn{\\textrm{ for } i = 1, \\ldots, n.}\n\\end{eqnarray*}\\]\n\nData: A sample from a population of 52 female song sparrows was studied over the course of a summer and their reproductive activities were recorded. In particular, the age and number of new offspring were recorded for each sparrow (Arcese et al, 1992). Let \\(Y = \\textrm{fledged}\\) (number of offspring), and \\(X =  \\textrm{age}\\) (age of mother).\nBased on the results from last week, test (through a frequentist approach):\n\n\\[H_0: \\beta_1 = \\beta_2 = 0 \\ \\ \\ \\ vs. \\ \\ \\ \\ H_1: \\beta_1 \\not= 0 , \\beta_2 \\not= 0 \\]\n\nFrom Statistical Inference by Garthwaite, Joliffe, and Jones answer the following (for the course the questions may be edited and/or sightly modified):\n\n\nQ 5.1: Let \\(X_1, \\ldots, X_n \\iid f_X(x; \\theta) = (1/\\theta) \\exp(-x/\\theta), \\ x&gt;0.\\) Using the result that \\(Y= 2 \\sum_{i=1}^n X_i/\\theta\\) has a \\(\\chi^2_{(2n)}\\) distribution, construct a confidence interval for \\(\\theta\\) based on the pivotal quantity \\(Y\\).\nQ 5.4: Let \\(X_1, \\ldots, X_n \\iid f_X(x; \\theta) = \\theta x^{\\theta-1}, \\ 0&lt;x&lt;1, \\ \\theta&gt;0.\\) Use the MLE for \\(\\theta\\) to find an approximate \\(100(1-\\alpha)%\\) confidence interval for \\(\\theta\\).\nQ 7.1: Suppose \\(X_1, \\ldots, X_n \\sim \\textrm{Poisson}(\\theta)\\). Let \\(p(\\theta)\\) be the prior distribution for \\(\\theta\\).\n\nGive an expression for the posterior distribution of \\(\\theta\\), \\(p(\\theta | \\bs{x})\\) ignoring the constants of proportionality (i.e. integrating constants).\nSuppose \\(Y\\) is an observation from a Poisson distribution with mean \\(n\\theta\\) and \\(p(\\theta)\\) is the prior distribution of \\(\\theta\\). Show that the posterior distribution is the same as in (a).\nDetermine that \\(T=\\sum_{i=1}^n X_i\\) is a sufficient statistic for \\(\\theta\\)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Tutorial 10  (Week 11)</span>"
    ]
  },
  {
    "objectID": "Tut11.html",
    "href": "Tut11.html",
    "title": "11  Tutorial 11 (Week 12)",
    "section": "",
    "text": "From Statistical Inference by Garthwaite, Joliffe, and Jones answer the following (for the course the questions may be edited and/or sightly modified):\n\n\nQ 7.3: Show that\n\\[f_X(x;\\theta) = \\frac{\\theta^2}{\\theta+1} (x+1) \\exp(-x \\theta), \\ \\ (x&gt;0)\\]\nis, for any \\(\\theta&gt;0\\) a p.d.f. Suppose \\(X_1, \\ldots, X_n \\iid f_X(x;\\theta)\\).\n\nWhat is the likelihood function (ignore proportionality constants)?\nGiven functions of \\(x_1, \\ldots, x_n\\), which are sufficient for \\(\\theta\\)?\nGive the conjugate prior distribution for \\(\\theta\\).\nObtain the posterior distribution for \\(\\theta\\) to proportionality using the conjugate prior and compare its form to the conjugate prior.\n\nQ 7.10: Prior information on the mean of \\(\\theta\\) for a Poisson distribution is such that the prior mean and prior variance of \\(\\theta\\) are both unity.\n\nDetermine the conjugate prior having these properties.\nFind the corresponding posterior density of \\(\\theta\\) given a random sample of \\(n\\) observations from a Poisson distribution.\nDerive expressions, in terms of \\(n\\), the sample observations, and the critical points of a \\(\\chi^2\\) distribution, for an (1-\\(\\alpha\\))100% equal-tailed credible interval for \\(\\theta\\).\n\nQ 7.15: Let \\(X_1, \\ldots, X_n \\iid \\textrm{exponential} \\ (\\theta); \\ E[X] = 1/\\theta\\). Suppose we wish to test \\(H_0: \\theta = 1\\) against \\(H_1: \\theta \\not= 1\\), where \\(P(H_0) = p\\) and \\(P(H_1) = 1-p\\). If the prior distribution for \\(\\theta\\), given \\(H_1\\) is:\n\\[p_1(\\theta | H_1) = \\beta^{\\alpha} \\theta^{\\alpha-1} \\exp(-\\beta \\theta)/\\Gamma(\\alpha), \\ \\ \\theta \\not=1.\\]\nDetermine the posterior odds \\(Q^* = \\frac{P(H_0|\\bs{x})}{P(H_1|\\bs{x})}\\).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Tutorial 11 (Week 12)</span>"
    ]
  }
]